{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,jsonify\n",
    "from flask import request\n",
    "import re\n",
    "import string \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nlp_id.postag import PosTag\n",
    "#import fasttext.util\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from tensorboard.plugins import projector\n",
    "from tensorflow import keras\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from bs4 import BeautifulSoup\n",
    "from flask_cors import CORS\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "factory_stemming = StemmerFactory()\n",
    "stemmer = factory_stemming.create_stemmer()\n",
    "app = Flask(__name__)\n",
    "#model machine learning download di \n",
    "#https://drive.google.com/file/d/1wl796Qa9LhE7dhQFIcSNx_A1gIQsKMCK/view?usp=sharing\n",
    "model = keras.models.load_model(\"E:/MODEL MACHINE LEARNING/LSTM-RNN/trial_error/model-lstm-1-rnn-150-v1\")\n",
    "postagger = PosTag()\n",
    "def remove_punct(text):\n",
    "    text_nonpunct = ''\n",
    "    text_nonpunct = re.sub('['+string.punctuation+']','',text)\n",
    "    return text_nonpunct\n",
    "\n",
    "def lower_token(tokens):\n",
    "    return [w.lower() for w in tokens]\n",
    "\n",
    "def cleanParagraph(paragraph):\n",
    "    paragraph_stopword_removed = stopword.remove(paragraph)\n",
    "    text_steammed = stemmer.stem(paragraph_stopword_removed)\n",
    "    return text_steammed\n",
    "\n",
    "def generateCleanText(lists):\n",
    "    clean_text = []\n",
    "    length_data = len(lists['content'])\n",
    "    for i in range(0,length_data):\n",
    "        lists['content'][i] = BeautifulSoup(lists['content'][i], \"lxml\").text\n",
    "        lists['content'][i] = lists['content'][i].replace('Liputan6.com, ','').replace('JAKARTA,KOMPAS.com','').replace('JAKARTA, KOMPAS.com','').replace('Liputan.com, Jakarta','')\n",
    "        lists['content'][i] = lists['content'][i].replace('kompas com','')\n",
    "        lists['content'][i] = re.sub(r\"\\d+\",\"\",lists['content'][i])\n",
    "        lists['content'][i] = re.sub('[^a-zA-Z]', ' ',lists['content'][i])\n",
    "        lists['content'][i] = lists['content'][i].replace('\\n','')\n",
    "        lists['content'][i] = lists['content'][i].lower()\n",
    "        clean_text.append(cleanParagraph(lists['content'][i]))\n",
    "    return clean_text\n",
    "\n",
    "def tokenizing(listData):\n",
    "    tokens = []\n",
    "    length_data = len(listData['text_clean'])\n",
    "    for i in range(0,length_data):\n",
    "        tokens.append(postagger.get_pos_tag(listData['text_clean'][i]))\n",
    "        listData['text_clean'][i] = remove_punct(listData['text_clean'][i])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "d_train = pd.read_csv('data/data_train.csv')\n",
    "d_train['tokens'] = tokenizing(d_train)\n",
    "d_test = pd.read_csv('data/data_test.csv')\n",
    "d_test['tokens'] = tokenizing(d_test)\n",
    "training_words = [word for tokens in d_train[\"tokens\"] for word in tokens]\n",
    "testing_words = [word for tokens in d_test[\"tokens\"] for word in tokens]\n",
    "all_training_words = training_words + testing_words\n",
    "training_sentence_lengths = [len(tokens) for tokens in d_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB),filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True,char_level=False)\n",
    "tokenizer.fit_on_texts(d_train[\"text_clean\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "CORS(app)\n",
    "@app.route('/',methods=['POST'])\n",
    "def predict():\n",
    "    MAX_SEQUENCE_LENGTH = 150\n",
    "    param = request.form.get('text')\n",
    "    data = []\n",
    "    data.append(param)\n",
    "    data_predict = pd.DataFrame(data,columns=['content'])\n",
    "    data_predict['content'][0] = remove_punct(data_predict['content'][0])\n",
    "    data_predict['content'][0] = re.sub(r\"\\d+\",\"\",data_predict['content'][0])\n",
    "    data_predict['content'][0] = re.sub('[^a-zA-Z]', ' ',data_predict['content'][0])\n",
    "    data_predict['content'][0] = data_predict['content'][0].lower()\n",
    "    data_predict['content'][0] = data_predict['content'][0].replace('\\n','')\n",
    "    data_predict_remove_stopword = stopword.remove(data_predict['content'][0])\n",
    "    data_predict['clean_text'] = stemmer.stem(data_predict_remove_stopword)\n",
    "    print(data_predict['clean_text'][0])\n",
    "    test_sequences = tokenizer.texts_to_sequences(data_predict['clean_text'].values)\n",
    "    print(test_sequences)\n",
    "    data_predict_pad = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    result = model.predict(data_predict_pad.reshape(1,data_predict_pad.shape[1]),batch_size=128)[0]\n",
    "    result_predicted = np.argmax(result)\n",
    "    if result_predicted == 0 :\n",
    "        return jsonify('News')\n",
    "    elif result_predicted == 1 :\n",
    "        return jsonify('Bola')\n",
    "    elif result_predicted == 2 :\n",
    "        return jsonify('Bisnis')\n",
    "    elif result_predicted == 3 :\n",
    "        return jsonify('Otomotif')\n",
    "    elif result_predicted == 4 :\n",
    "        return jsonify('Tekno')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
